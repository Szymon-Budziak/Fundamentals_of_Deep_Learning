{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will take a detour away from stand-alone pieces of data such as still images, to data that is dependent on other data items in a sequence. For our example, we will use text sentences. Elements in the data have a relationship with what comes before and what comes after, and this fact requires a different aproach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headline Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all seen text predictors in applications lilke the search bars, on cell phones, or in text editors that provide autocompletion of sentences. Many of the good text predictor models are trained on very large datasets and take a lot of time and/or processing power to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset consists of headlines from the New York Times newspaper of several months. We wil start by reading in all the headlines from the articles. The articles are in CSV files, so we can use *pandas* to read them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9335"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyt_dir = 'Data/nyt_dataset/'\n",
    "\n",
    "all_headlines = []\n",
    "for filename in os.listdir(nyt_dir):\n",
    "    if 'Articles' in filename:\n",
    "        # Read in all the data from the CSV file\n",
    "        headlines_df = pd.read_csv(nyt_dir + filename)\n",
    "        # Add all of the headlines to our list\n",
    "        all_headlines.extend(list(headlines_df.headline.values))\n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I Stand  With the ‘She-Devils’',\n",
       " 'Trump’s Birth Control Problems',\n",
       " 'What’s the Craziest Thing You’ve Ever Found in a Xerox Machine?',\n",
       " 'U.S. Allies’ Conflict Is ISIS’ Gain',\n",
       " '$1.5 Trillion Plan on Infrastructure, but Not a Lot of Funding or Details',\n",
       " 'Mueller Zeros In on a Trump Tower Cover Story',\n",
       " ' With Speech, A ‘Dreamers’ Rift Deepens',\n",
       " 'At the Start',\n",
       " '‘The Assassination of Gianni Versace’ Episode 3: Death or Disgrace?',\n",
       " 'Britain’s Model for Outsourcing Services May Be Cracking',\n",
       " 'Unknown',\n",
       " 'Real Friends vs. Facebook Friends',\n",
       " 'For Kurds and Allies in Syria, U.S. Vow of Support Eases Fears on Turkey',\n",
       " 'Should Schools Teach You How to Be Happy?',\n",
       " 'Jessica Williams and Phoebe Robinson, Onstage',\n",
       " 'Autocrats Steamroll Opponents With No Objections From U.S.',\n",
       " '32 Billion',\n",
       " 'A Volcanic Idea for Cooling the Earth',\n",
       " 'After the man was treated for a bad infection, tests indicated he was getting better. So why was he feeling weaker by the day?',\n",
       " 'Republicans Pack Campus Social Agenda Into Broad Education Bill']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important part of natural language processing (NLP) tasks (where computers deal with language), is processing tesxt in a way that coputers can understand it. We are going to take each of the words that appears in our dataser and represent it with a number. This will be part of a process calles **tokenization**.\n",
    "\n",
    "Before we do that, we need to make sure we have good data. There are some headlines that are listed as \"Unknown\". We do not want these items in our training set, so we will filter them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8603"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all headlines with the value of \"Unknown\"\n",
    "all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I Stand  With the ‘She-Devils’',\n",
       " 'Trump’s Birth Control Problems',\n",
       " 'What’s the Craziest Thing You’ve Ever Found in a Xerox Machine?',\n",
       " 'U.S. Allies’ Conflict Is ISIS’ Gain',\n",
       " '$1.5 Trillion Plan on Infrastructure, but Not a Lot of Funding or Details',\n",
       " 'Mueller Zeros In on a Trump Tower Cover Story',\n",
       " ' With Speech, A ‘Dreamers’ Rift Deepens',\n",
       " 'At the Start',\n",
       " '‘The Assassination of Gianni Versace’ Episode 3: Death or Disgrace?',\n",
       " 'Britain’s Model for Outsourcing Services May Be Cracking',\n",
       " 'Real Friends vs. Facebook Friends',\n",
       " 'For Kurds and Allies in Syria, U.S. Vow of Support Eases Fears on Turkey',\n",
       " 'Should Schools Teach You How to Be Happy?',\n",
       " 'Jessica Williams and Phoebe Robinson, Onstage',\n",
       " 'Autocrats Steamroll Opponents With No Objections From U.S.',\n",
       " '32 Billion',\n",
       " 'A Volcanic Idea for Cooling the Earth',\n",
       " 'After the man was treated for a bad infection, tests indicated he was getting better. So why was he feeling weaker by the day?',\n",
       " 'Republicans Pack Campus Social Agenda Into Broad Education Bill',\n",
       " 'The Greatest Plays You Don’t Remember']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to remove punctuation and make our sentences all lower case, because this will make our model easier to train. For our purposes, there is little or no difference between a line ending with \"!\" or \"?\" or whateve words are capitalized, as in \"The\" or lower-case, as in \"the\". With fewer unique tokens, our model will be easier to train.\n",
    "\n",
    "We could filter our sentences prior to tokenization, but we do not need to becasue this ccan be all done using the Keras `Tokenizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, our dataset consists of a set of headlines, each made up of a series of words. We want to give our model a way of represenmting those words in a way that it can understand. With tokenization, we separate a piece of text into smaller chunks (tokens), which in this case are words. Each unique word is then assigned a number, as this is a way that our model can understand the data. Keras has a class that will help us tokenize our data:\n",
    "\n",
    "```python\n",
    "tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    char_level=False,\n",
    "    oov_token=None,\n",
    "    document_count=0,\n",
    "    **kwargs\n",
    "    )\n",
    "```\n",
    "\n",
    "Taking a look at the `Tokenizer` class in Keras, we see the default values are already set up for our use case. The `filters` string already removes punctuation and the `lower` flag sets words to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:  11753\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the words in our headlines\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_headlines)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(\"Total words: \", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the \"word_index\" dictionary to see how the tokenizer saves the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 2, 'plan': 82, 'man': 137, 'panama': 2823, 'canal': 9037}\n"
     ]
    }
   ],
   "source": [
    "# Print a subset of the word_index dictionary created by Tokenizer\n",
    "subset_dict = {key: value for key, value in tokenizer.word_index.items()\\\n",
    "              if key in ['a', 'man', 'a', 'plan', 'a', 'canal', 'panama']}\n",
    "print(subset_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `texts_to_sequences` method to see how the tokenizer saves the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2], [137], [2], [82], [2], [9037], [2823]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['a', 'man', 'a', 'plan', 'a', 'canal', 'panama'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tokenized the data, turning each word into representative number, we will create sequences of tokens from the headlines. These sequences are what we will train our deep learning model on.\n",
    "\n",
    "For example. let's take the headline, \"nvidia launches ray tracing gpus\". Each word is going to be replaced by a corresponding number, for instance: nvidia - 5, launches - 22, ray - 94, tracing - 16, gpus - 102. The full sequence would be: [5, 22, 94, 16, 102]. However, it is also valuable to train on the smaller sequences within the headline, such as \"nvidia launches\". We will take each headline and create a set of sequences to fill our dataset. Nest, let's use our tokenizer to convert our headlines to a set of sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i stand', 'i stand with', 'i stand with the', 'i stand with the ‘she', 'i stand with the ‘she devils’']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[30, 314],\n",
       " [30, 314, 11],\n",
       " [30, 314, 11, 1],\n",
       " [30, 314, 11, 1, 3395],\n",
       " [30, 314, 11, 1, 3395, 5242]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert data to sequence of tokens\n",
    "input_sequences = []\n",
    "for line in all_headlines:\n",
    "    # Convert our headline into a sequence of tokens\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    \n",
    "    # Create a series of sequences for each headline\n",
    "    for i in range(1, len(token_list)):\n",
    "        partial_sequence = token_list[:i+1]\n",
    "        input_sequences.append(partial_sequence)\n",
    "        \n",
    "print(tokenizer.sequences_to_texts(input_sequences[:5]))\n",
    "input_sequences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our sequences are of various length. For our model to be able to train on the data, we need to make all the sequences the same length. To do this we will add padding to the sequences. Keras has a built-in `pad_sequences` method that we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        30, 314], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine max sequence length\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "# Pad all sequences with zeros at the beginning to make them all max length\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len,\n",
    "                                         padding='pre'))\n",
    "input_sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating predictors and Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to split up our sequences into predictors and a target. The last word of the sequence will be our target, and the first word of the sequence will be our predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 314,   11,    1, 3395, 5242], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictors are every word except the last\n",
    "predictors = input_sequences[:, :-1]\n",
    "# Labels are the last word\n",
    "labels = input_sequences[:, -1]\n",
    "labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are predicting one word out of our possible total vocabulary. Instead of the network predicting scalar numbers, we will have it predict binary categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our model, we are going to use a couple of new layers to deal with our sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding Layer**\n",
    "\n",
    "Our first layer is an embedding layer:\n",
    "```python\n",
    "model.add(Embedding(input_dimension, output_dimension, input_length = input_len))\n",
    "```\n",
    "This layer will take the tokenized sequences and will learn an embedding for all of the words in the training dataset. Mathematically, embeddings work the same way as a neuron in a neutral network, but conceptually their goal is to reduce the number of dimensions for some or all of the features. In this case, it will represent each word as a vector, and the information within that vector will contain the relationships between each word.\n",
    "\n",
    "**Long Short Term Memory Layer**\n",
    "\n",
    "Our next layer, is a long short term memory layer (LSTM). An LSTM is a type of a recurrent neural network or RNN. Unlike traditional feed-forward networks, recurrent networks have loops in them, allowing information to persist.\n",
    "\n",
    "New information (x) gets passed in to the network, which splits our prediction (h). Additionally, information from that layer gets saved, and used as input for the next prediction. When a new piece of data (x) is fed into the network, that network both spits out a prediction (h) and also passes some information along to the next layer. That next layer gets another piece of data, but gets to learn from the layer before it as well.\n",
    "\n",
    "Traditional RNNs suffer from the issure of more recent information contributing more than information from further back. LSTMs are a special type of recurrent layer that are able to learn and retain longer term information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is max sequence length -1, as we have removed the last word for the label\n",
    "input_len = max_sequence_len - 1\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add input embedding layer\n",
    "model.add(Embedding(total_words, 10, input_length = input_len))\n",
    "\n",
    "# Add LSTM layer with 100 units\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Add output layer\n",
    "model.add(Dense(total_words, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 27, 10)            117530    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 11753)             1187053   \n",
      "=================================================================\n",
      "Total params: 1,348,983\n",
      "Trainable params: 1,348,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile our model with categorical crossentroy (as before), as we are categorically predictiong one word from our total vocabulary. In this case, we are not going to use accuracy as a metric, because text prediction is not measured as being more or less acurate in the same way as image classification.\n",
    "\n",
    "We are also going to select particular optimizer that is well suited for LSTM tasks, called *Adam optimizer*. What is important for optimizers is that different optimizers can be better for different deep learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important thing to notice, is that we don't have a training or validation accuracy score in this case. This reflects our different problem of text prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1666/1666 [==============================] - 35s 20ms/step - loss: 7.8853\n",
      "Epoch 2/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 7.4787\n",
      "Epoch 3/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 7.2948\n",
      "Epoch 4/30\n",
      "1666/1666 [==============================] - 33s 20ms/step - loss: 7.0956\n",
      "Epoch 5/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 6.8735\n",
      "Epoch 6/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 6.6226\n",
      "Epoch 7/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 6.3702\n",
      "Epoch 8/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 6.1205\n",
      "Epoch 9/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 5.8809\n",
      "Epoch 10/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 5.6477\n",
      "Epoch 11/30\n",
      "1666/1666 [==============================] - 33s 20ms/step - loss: 5.4281\n",
      "Epoch 12/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 5.2217\n",
      "Epoch 13/30\n",
      "1666/1666 [==============================] - 33s 20ms/step - loss: 5.0249\n",
      "Epoch 14/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 4.8360\n",
      "Epoch 15/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 4.6626\n",
      "Epoch 16/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 4.4986\n",
      "Epoch 17/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 4.3401\n",
      "Epoch 18/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 4.1898\n",
      "Epoch 19/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 4.0568\n",
      "Epoch 20/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 3.9245\n",
      "Epoch 21/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 3.8085\n",
      "Epoch 22/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 3.6933\n",
      "Epoch 23/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 3.5866\n",
      "Epoch 24/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 3.4915\n",
      "Epoch 25/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 3.3961\n",
      "Epoch 26/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 3.3093\n",
      "Epoch 27/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 3.2258\n",
      "Epoch 28/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 3.1483\n",
      "Epoch 29/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 3.0736\n",
      "Epoch 30/30\n",
      "1666/1666 [==============================] - 34s 20ms/step - loss: 2.9989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f56fef507c0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictors, labels, epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the loss decreased over the course of training. We could train our model further to decrease the loss, but that would takse some time, and we are not looking for a perfect text predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make predictions, we will need to start with a seed text, and prepare it in the same way we prepared our dataset. This will mean tokenizing and padding. We will create function to be able to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(seed_text):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    prediction = model.predict_classes(token_list, verbose=0)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predict_next_token('today in new york')\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our tokenizer to decode the predicted word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are able to predict new words, let's create a function that can predict headlines of more than just one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates a new headline of arbitrary length\n",
    "def generate_headlines(seed_text, next_words=1):\n",
    "    for _ in range(next_words):\n",
    "        # Predict next token\n",
    "        prediction = predict_next_token(seed_text)\n",
    "        \n",
    "        # Convert token to word\n",
    "        next_word = tokenizer.sequences_to_texts([prediction])[0]\n",
    "        \n",
    "        # Add next word to the headline. This headlines will be used in the next pass of the loop.\n",
    "        seed_text += \" \" + next_word\n",
    "        \n",
    "    # Return headline as title_case\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Washington Dc Is The Trump Right Not Too\n",
      "Today In New York In New ‘America Kong Brooklyn’\n",
      "The School District Has The New York Times Affects\n",
      "Crime Has Become A Brick Wall On Top\n"
     ]
    }
   ],
   "source": [
    "seed_texts = ['washington dc is',\n",
    "             'today in new york',\n",
    "             'the school district has',\n",
    "             'crime has become']\n",
    "for seed in seed_texts:\n",
    "    print(generate_headlines(seed, next_words=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite of the 30 epochs of training we can notice that most of the headlines make some kind of grammatical sense, but don't necessarily indicate a good contextual understanding. The results might improve somewhat by running more epochs. We can do this by running the training `fit` cell again (and again!) to train antoher 30 epochs each time. We should see that the loss value go down. After that try the tests again and results can vary quite a bit!\n",
    "\n",
    "Other improvement would be to try using pretrained embeddings with Word2Vec or GloVe, rather than learning them during training as we did with the Keras Embadding layer.\n",
    "\n",
    " Ultimately, however, NLP has moved beyond simple LSTM models to Tranformer-based pre-trained models, which are able to learn language context from huge amounts of textual data such as Wikipedia. These pre-trained models are then used as a starting point for transfer learning to solve NLP tasks such as the one we just tried for text completion.\n",
    " \n",
    " We have successfully trained a model to predict words in a headline and used that model to create headlines of various lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear the GPU memory\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
