{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook we built and trained a simple model to classify ASL images. The model was able to learn how to correctly classify the training dataset with very high accuracy, but it did not perform nearly as well on validation dataset. This is called overfitting.\n",
    "\n",
    "*Overfitting* is a behaviour of not generalizing well to non-training data (validation dataset). Here we will use a popular kind of model called a convolutional neural network that is especially good for reading images and classifying them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV files\n",
    "train_df = pd.read_csv('Data/mnist/sign_mnist_train.csv')\n",
    "valid_df = pd.read_csv('Data/mnist/sign_mnist_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out target values\n",
    "y_train = train_df['label']\n",
    "y_valid = valid_df['label']\n",
    "del train_df['label']\n",
    "del valid_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out image vectors\n",
    "x_train = train_df.values\n",
    "x_valid = valid_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn scalar targets into binary categories\n",
    "num_classes = 24\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_valid = keras.utils.to_categorical(y_valid, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize image data\n",
    "x_train = x_train / 255\n",
    "x_valid = x_valid / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping Images for a CNN (Convolutiona Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27455, 784), (7172, 784))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual pictures in our dataset are in the format of long lists of 784 pixels.\n",
    "\n",
    "In this format we do not have all the information abut which pixels are near each other. Because of this, we can't apply convolutions that will detect featuers. Let's reshape our dataset so that they are in 28x28 pixel format. This will allow out convolutions to associate groups of pixels and detect important features.\n",
    "\n",
    "Note: In the first convolutional layer of the model, we need to have not only the height and width of the image, but also the number of *color channels*. Our images are grayscale, so we will just have 1 channel.\n",
    "\n",
    "That means that we need to convert the current shape `(274555, 784)` to `(27455, 28, 28, 1)`. As a convenience, we can pass the `reshape` method a `-1` for any dimension we wish to remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_valid = x_valid.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27455, 28, 28, 1), (7172, 28, 28, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Convolutional Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thsese days, many data scientist start their projects by borrowing model properties from a similar project. Assuming the problem is not totally unique, there is a great chance that people have created models that will perform well which are posted in online repositories like `TensorFlow Hub` and the `NGC Catalog`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense, Conv2D, MaxPool2D, Flatten,\n",
    "                                     Dropout, BatchNormalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(75, (3, 3), strides=1, padding='same', \n",
    "                 activation = 'relu', input_shape=(28,28,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
    "model.add(Conv2D(50, (3,3), strides=1, padding='same', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
    "model.add(Conv2D(25, (3,3), strides=1, padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2,2), strides=2, padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conv2D**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are our 2D convolutional layers. Small kernels will go over the input image and detect features that are important for classification. Earlier convolutions in the model will detect simple features such as lines. Later convolutions will detect more complex features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv2D layer:\n",
    "```Python\n",
    "model.add(Conv2D(75, (3,3), strides=1, padding='same', ...))\n",
    "```\n",
    "75 referes to the number of filters that will be learned. (3,3) refers to the size of those filters. Strides refer to the step size that the filter will take as it passes over the image. Padding refers to whether the output image that's created from the filter will match the size of the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BatchNormalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like normalizin our inputs, batch normalization scales the values in the hidden layers to improve training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MaxPool2D**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max pooling takes an image and essentially shrinks it to a lower resolution. It does this to help the model be robust to translation (objects moving side to side) and also makes our model faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is a technique for preventing overfitting. Dropout randomly selects a subset of neurons and turns them off, so that they do not participate in forward or backward propagation in that particular pass. This helps to make sure that the network is robust and redundant and does not rely on any one area to come up with answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flatten**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten takes the output of one layer which is multidimensional, and flattens it into a one-dimensional array. The ouput is calles a *feature vector* and will be connected to the final classification layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dense**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first dense layer (512 units) takes the feature vector as input and learns which features will contribute to a particular classification. The second dense layer (24 units) is the final classification layer that outputs our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: This model has fewer trainable parameters than the model in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 75)        750       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 28, 28, 75)        300       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 75)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 50)        33800     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 50)        200       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 25)          11275     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 7, 7, 25)          100       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 25)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               205312    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                12312     \n",
      "=================================================================\n",
      "Total params: 264,049\n",
      "Trainable params: 263,749\n",
      "Non-trainable params: 300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "858/858 [==============================] - 48s 55ms/step - loss: 0.3030 - accuracy: 0.9066 - val_loss: 0.3373 - val_accuracy: 0.8879\n",
      "Epoch 2/20\n",
      "858/858 [==============================] - 45s 52ms/step - loss: 0.0216 - accuracy: 0.9934 - val_loss: 1.0689 - val_accuracy: 0.7494\n",
      "Epoch 3/20\n",
      "858/858 [==============================] - 44s 52ms/step - loss: 0.0104 - accuracy: 0.9967 - val_loss: 0.3004 - val_accuracy: 0.9186\n",
      "Epoch 4/20\n",
      "858/858 [==============================] - 45s 52ms/step - loss: 0.0073 - accuracy: 0.9975 - val_loss: 0.3708 - val_accuracy: 0.9137\n",
      "Epoch 5/20\n",
      "858/858 [==============================] - 45s 52ms/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.1754 - val_accuracy: 0.9593\n",
      "Epoch 6/20\n",
      "858/858 [==============================] - 45s 52ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.2130 - val_accuracy: 0.9593\n",
      "Epoch 7/20\n",
      "858/858 [==============================] - 45s 53ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 0.2102 - val_accuracy: 0.9511\n",
      "Epoch 8/20\n",
      "858/858 [==============================] - 45s 52ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.2669 - val_accuracy: 0.9462\n",
      "Epoch 9/20\n",
      "858/858 [==============================] - 45s 52ms/step - loss: 0.0044 - accuracy: 0.9989 - val_loss: 0.3031 - val_accuracy: 0.9656\n",
      "Epoch 10/20\n",
      "858/858 [==============================] - 45s 52ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.3676 - val_accuracy: 0.9417\n",
      "Epoch 11/20\n",
      "858/858 [==============================] - 44s 52ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.2257 - val_accuracy: 0.9674\n",
      "Epoch 12/20\n",
      "858/858 [==============================] - 45s 52ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 0.2314 - val_accuracy: 0.9625\n",
      "Epoch 13/20\n",
      "858/858 [==============================] - 45s 52ms/step - loss: 8.3856e-04 - accuracy: 0.9996 - val_loss: 0.2350 - val_accuracy: 0.9679\n",
      "Epoch 14/20\n",
      "858/858 [==============================] - 44s 52ms/step - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.1650 - val_accuracy: 0.9763\n",
      "Epoch 15/20\n",
      "858/858 [==============================] - 45s 52ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.3587 - val_accuracy: 0.9449\n",
      "Epoch 16/20\n",
      "858/858 [==============================] - 44s 51ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.3374 - val_accuracy: 0.9558\n",
      "Epoch 17/20\n",
      "858/858 [==============================] - 44s 52ms/step - loss: 9.9865e-04 - accuracy: 0.9997 - val_loss: 0.2157 - val_accuracy: 0.9644\n",
      "Epoch 18/20\n",
      "858/858 [==============================] - 44s 52ms/step - loss: 8.0970e-04 - accuracy: 0.9997 - val_loss: 0.1405 - val_accuracy: 0.9700\n",
      "Epoch 19/20\n",
      "858/858 [==============================] - 46s 53ms/step - loss: 5.7555e-04 - accuracy: 0.9999 - val_loss: 0.3554 - val_accuracy: 0.9629\n",
      "Epoch 20/20\n",
      "858/858 [==============================] - 45s 53ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.3615 - val_accuracy: 0.9532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8b997e5580>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=20, verbose=1, validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this model is significantly improved than the model in previous notebook. The training accuracy is very high, and the validation accuracy has improved as well. This is a great result, as all we had to do was swap in a new model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we utilized several new kinds of layers to implement a CNN, which perforemd better that the more simple model used in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear GPU memory\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
